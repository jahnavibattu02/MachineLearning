# Naive Bayes Classifier
Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.  

## Assumption:
The fundamental Naive Bayes assumption is that each feature makes an:  
1. Independent - We assume that no pair of features are dependent  
2. Equal contribution to the outcome - each feature is given the same weight(or importance)    

## Bayes’ Theorem:  
It finds the probability of an event occurring given the probability of another event that has already occurred. Bayes’ theorem is stated mathematically as the following equation:
```
 P(A|B) = \frac{P(B|A) P(A)}{P(B)} 
 ```
- Basically, we are trying to find probability of event A, given the event B is true. Event B is also termed as evidence.
- P(A) is the priori of A (the prior probability, i.e. Probability of event before evidence is seen). The evidence is an attribute value of an unknown instance(here, it is event B).
- P(A|B) is a posteriori probability of B, i.e. probability of event after evidence is seen. 

In **Gaussian Naive Bayes**, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution.   
Other popular Naive Bayes classifiers are:

1. **Multinomial Naive Bayes:**
 Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.
2. **Bernoulli Naive Bayes:** 
In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence(i.e. a word occurs in a document or not) features are used rather than term frequencies(i.e. frequency of a word in the document).

## Machine Learning WorkFlow:
1. Downloading a real-world dataset
2. Preparing a dataset for training
   - Create a train/test/validation split
   - Identify input and target columns
   - Identify numeric and categorical columns
   - Impute (fill) missing numeric values
   - Scale numeric values to the $(0, 1)$ range
   - Encode categorical columns to one-hot vectors
3. Model Defining.
4. Training the Model.
5. Making predictions on single inputs


<img src="https://www.deepnetts.com/blog/wp-content/uploads/2019/02/SupervisedLearning.png" width="400">
## Code:
            from sklearn.naive_bayes import GaussianNB
            gnb = GaussianNB()
            gnb.fit(X_train, y_train)

            # making predictions on the testing set
            y_pred = gnb.predict(X_test)
  
